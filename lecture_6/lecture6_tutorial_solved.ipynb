{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial not ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "import scipy.stats\n",
    "import scipy.io\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from scipy.ndimage.filters import maximum_filter1d\n",
    "from scipy.ndimage.filters import minimum_filter1d\n",
    "from scipy.ndimage.filters import convolve1d\n",
    "from scipy.signal import medfilt\n",
    "from six.moves import urllib\n",
    "import os\n",
    "import sklearn.decomposition\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 2-photon data\n",
    "if not os.path.isfile('data/MP019.npy'):\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    with urllib.request.urlopen('http://www.gatsby.ucl.ac.uk/~cstringer/MP019.npy') as response:\n",
    "        with open('data/MP019.npy','wb') as f:\n",
    "            f.write(response.read())\n",
    "\n",
    "dat = np.load('data/MP019.npy')\n",
    "dat = dat.item() # <- dat is a dict so we have to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA preprocessing\n",
    "# we need to baseline and deconvolve the calcium fluorescence data\n",
    "\n",
    "from scipy.ndimage import filters\n",
    "\n",
    "F = dat['F']\n",
    "\n",
    "ops = {}\n",
    "# baseline fluorescence traces recorded with 2p calcium imaging\n",
    "ops['sig_baseline'] = 10  # adjust this to get desired result\n",
    "ops['win_baseline'] = 80 # adjust this to get desired result\n",
    "ops['fs'] = 5\n",
    "\n",
    "sig = ops['sig_baseline']\n",
    "win = int(ops['win_baseline']*ops['fs'])    \n",
    "\n",
    "# smooth the data first, to remove high-frequency noise\n",
    "Flow = filters.gaussian_filter(F,    [0., sig])\n",
    "\n",
    "# now run a minimum filter\n",
    "Flow = filters.minimum_filter1d(Flow,    win)\n",
    "\n",
    "# now run a maximum filter\n",
    "Flow = filters.maximum_filter1d(Flow,    win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the baseline for neuron iN\n",
    "iN = 100\n",
    "timerange = np.arange(0, 1000)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(F[iN,timerange])\n",
    "plt.plot(Flow[iN,timerange])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we're happy with the parameters, run the baselined data through deconvolution\n",
    "from suite2p import dcnv\n",
    "\n",
    "ops['tau'] = 1. # timescale of gcamp sensor\n",
    "ops['baseline'] = 'none' # we're baselining the data ourselves\n",
    "F0 = F - Flow\n",
    "sp = dcnv.oasis(F0, ops) # deconvolve the baselined traces using OASIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iN = 100\n",
    "timerange = np.arange(0, sp.shape[1])\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(F0[iN,timerange])\n",
    "plt.plot(sp[iN,timerange])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the fluorescence from the deconvolved using the exponential convolution filter\n",
    "flt  = np.exp(-np.arange(0,3*ops['fs']*ops['tau']) / (ops['tau'] * ops['fs']))\n",
    "plt.plot(flt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the reconstruction for neuron iN\n",
    "iN = 100\n",
    "timerange = np.arange(0, 300)\n",
    "Frec = convolve1d(sp, flt, axis=1)\n",
    "\n",
    "tlag = int(len(flt)/2) # timelag from centering convolve1d\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(F0[iN,timerange[tlag:]])\n",
    "plt.plot(Frec[iN,timerange])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to build some models! \n",
    "# use T consecutive samples to predict the T+1-th sample\n",
    "\n",
    "# normalize spike data\n",
    "# subtract mean for each neuron\n",
    "sp  = sp - sp.mean(axis=1)[:,np.newaxis]\n",
    "# divide by the global standard deviation\n",
    "sp = sp/np.std(sp)\n",
    "\n",
    "# we will do this prediction one neuron at a time\n",
    "iN = 1\n",
    "nt = sp.shape[1]\n",
    "\n",
    "# let's make windows of size k (where k is how much history we consider)\n",
    "k = 100\n",
    "x = np.zeros((nt-k-2,k+2))\n",
    "for n in range(x.shape[0]):\n",
    "    x[n,:] = sp[iN, n:n+k+2]\n",
    "    \n",
    "# we predict the last sample\n",
    "y = x[:, k+1]\n",
    "# from the first k samples\n",
    "x = x[:, :k]\n",
    "\n",
    "# for regression we need the covariance of the inputs\n",
    "covX = x.T @ x / x.shape[0]\n",
    "# and the cross-covariance between output and input\n",
    "xty = x.T @ y / x.shape[0]\n",
    "\n",
    "# linalg.solve(a,b) solves the equations aw = b, so w = a^-1 * b, which is the solution to our regression problem\n",
    "w = np.linalg.solve(covX, xty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sp[iN,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's plot the filter w, the raw data, and the prediction\n",
    "\n",
    "# we will make a small function that computes the variance explained\n",
    "def variance_explained(y,x,w):\n",
    "    ypred = x @ w\n",
    "    varres = ((y - ypred)**2).mean()\n",
    "    varexp = 1 - varres / y[:-1].var()\n",
    "    return varexp\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(w)\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot(y[:500])\n",
    "ax2.plot(x[:500,:] @ w)\n",
    "plt.show()\n",
    "\n",
    "print('variance explained is %2.2f'%(variance_explained(y,x,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the covariance in time is a circulant matrix which reflects the autocorrelation function\n",
    "plt.imshow(covX)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# circulant matrices have PCs which are Fourier modes\n",
    "svx,ux = scipy.sparse.linalg.eigsh(covX,k=covX.shape[1])\n",
    "fig = plt.figure(figsize=(16,2))\n",
    "for k in range(10):\n",
    "    ax = fig.add_subplot(1,10,k+1)\n",
    "    ax.plot(ux[:,ux.shape[1]-1-k])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single neurons are noisy. Let's combine this with PCA\n",
    "# decompose data into PCs, predict the PCs\n",
    "\n",
    "# compute principal components\n",
    "nPCs = 100 # how many PCs to keep\n",
    "sv,u  = scipy.sparse.linalg.eigsh(sp @ sp.T, k=nPCs)\n",
    "\n",
    "u = u[:, ::-1] # PCs come in reverse order\n",
    "# project data into PCs\n",
    "tracePC = u.T @ sp\n",
    "\n",
    "# to reconstruct the data and predictions ,we will do\n",
    "# sp_rec = u @ tracePC\n",
    "\n",
    "# the following simply copies the code from above, in a loop for each PC\n",
    "k = 30\n",
    "tracePCrec = np.zeros((nPCs,nt-k-1)) # preallocate reconstructed traces\n",
    "E = np.zeros(nPCs)\n",
    "for iN in range(nPCs):\n",
    "    # let's make windows of size k (where k is how much history we consider)\n",
    "    x = np.zeros((nt-k-1,k+1))\n",
    "    for n in range(nt-k-1):\n",
    "        x[n,:] = tracePC[iN, n:n+k+1]\n",
    "    # we predict the last sample\n",
    "    y = x[:, k]\n",
    "    # from the first k samples\n",
    "    x = x[:, :k]\n",
    "    # for regression we need the covariance of the inputs\n",
    "    covX = x.T @ x / x.shape[0]\n",
    "    # and the cross-covariance between output and input\n",
    "    xty = x.T @ y / x.shape[0]\n",
    "    # linalg.solve(a,b) solves the equations aw = b, so w = a^-1 * b, which is the solution to our regression problem\n",
    "    w = np.linalg.solve(covX, xty)\n",
    "    # what is the reconstruction of the PC\n",
    "    tracePCrec[iN, :] = x @ w\n",
    "    E[iN] = 1-((x@w - y)**2).mean()/y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much variance is explained in each PC\n",
    "plt.plot(E)\n",
    "print('average explained variance is %2.2f'%(E.mean()))\n",
    "plt.show()\n",
    "# why do you think it decays as a function of PC#. Make a plot to show it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the reconstructions\n",
    "iN = 0\n",
    "NT = tracePC.shape[1]\n",
    "timerange = np.arange(0, NT-k-1) # plot the whole thing?\n",
    "#timerange = np.arange(0, 100) zoom in\n",
    "plt.plot(tracePC[iN, k+timerange]) # we are predicting starting at timepoint k\n",
    "plt.plot(tracePCrec[iN, timerange])\n",
    "plt.show()\n",
    "\n",
    "y = tracePC[iN, k+timerange]\n",
    "ypred = tracePCrec[iN, timerange]\n",
    "E0 = 1 - ((y-ypred)**2).mean()/y.var()\n",
    "print('explained variance for PC %d on this segment is %2.2f'%(iN, E0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[:1000])\n",
    "plt.plot(ypred[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we use this to predict single neuron activity? \n",
    "sp_rec = u @ tracePCrec\n",
    "\n",
    "# let's plot the reconstructions\n",
    "iN = 100\n",
    "NT = tracePC.shape[1]\n",
    "timerange = np.arange(0, NT-k-1) # plot the whole thing?\n",
    "#timerange = np.arange(0, 100) zoom in\n",
    "plt.plot(sp[iN, k+timerange]) # we are predicting starting at timepoint k\n",
    "plt.plot(sp_rec[iN, timerange])\n",
    "plt.show()\n",
    "\n",
    "y = sp[iN, k+timerange]\n",
    "ypred = sp_rec[iN, timerange]\n",
    "E = 1 - ((y-ypred)**2).mean()/y.var()\n",
    "print(E)\n",
    "\n",
    "# we did not predict as much! what gives? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# can we reconstruct the single neuron filters, based on the PC filters? \n",
    "\n",
    "# how about using this to predict one neuron from other neurons? can we reconstruct those filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# how many timesamples in the past do we need? make AR(1), AR(2) models. \n",
    "# go back above and replace k with 1, 2, 3, etc where does prediction saturate? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# Try basis functions instead with timescales of .5, 1, 2, 4 and 8 frames. \n",
    "# filter the PCs with exponential filters, predict from those\n",
    "# no need to window anymore for this: each exponential filter should be applied convolutionally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# check out the filters learnt: reconstruct them with the basis functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "# now let's see if we can predict a PC from other PCs. why would we even try this if the PCs are orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add pupil information. First predict only from the pupil, then try basis functions on the pupil\n",
    "pupil = dat['pupil'].flatten()\n",
    "plt.plot(pup)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the pupil\n",
    "\n",
    "sp -= sp.mean(axis=1)[:,np.newaxis]\n",
    "pupil -= pupil.mean()\n",
    "nt = pupil.shape[0]\n",
    "# use multiple timescales of the pupil\n",
    "nbasis = 10\n",
    "pbasis = np.zeros((nt,nbasis))\n",
    "for n in range(nbasis):\n",
    "    if n>0:\n",
    "        pbasis[:,n] = gaussian_filter1d(pupil, n*8)\n",
    "    else:\n",
    "        pbasis[:,n] = pupil\n",
    "        \n",
    "w = np.linalg.solve(pbasis.T @ pbasis, pbasis.T @ sp.T)\n",
    "\n",
    "sp_pred = w.T @ pbasis.T\n",
    "\n",
    "iN = 100\n",
    "plt.plot(sp[iN, :])\n",
    "plt.plot(sp_pred[iN, :])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPCs = 100 # how many PCs to keep\n",
    "sv,u  = scipy.sparse.linalg.eigsh(sp @ sp.T, k=nPCs)\n",
    "\n",
    "u = u[:, ::-1] # PCs come in reverse order\n",
    "\n",
    "tracePC = u.T@sp\n",
    "\n",
    "w = np.linalg.solve(pbasis.T @ pbasis, pbasis.T @ tracePC.T)\n",
    "\n",
    "tracePCrec = w.T @ pbasis.T\n",
    "\n",
    "iN = 0\n",
    "timerange=4000+np.arange(0,1000).astype(int)\n",
    "plt.plot(v[iN, timerange])\n",
    "plt.plot(v_pred[iN, timerange])\n",
    "plt.show()\n",
    "\n",
    "varexp_pupil = 1-((tracePC-tracePCrec)**2).sum(axis=1) / ((tracePC)**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting from the face PCs is better than the basis function pupil prediction\n",
    "face = dat['faceSVD']\n",
    "\n",
    "w = np.linalg.solve(face @ face.T, face @ tracePC.T)\n",
    "\n",
    "tracePCrec = w.T @ face\n",
    "\n",
    "iN = 0\n",
    "timerange=4000+np.arange(0,1000).astype(int)\n",
    "plt.plot(tracePC[iN, timerange])\n",
    "plt.plot(tracePCrec[iN, timerange])\n",
    "plt.show()\n",
    "\n",
    "varexp_face = 1-((tracePC-tracePCrec)**2).sum(axis=1) / ((tracePC)**2).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(E[:30]) #(<-- how much you can predict from previous timepts)\n",
    "plt.plot(varexp_face[:30])\n",
    "plt.plot(varexp_pupil[:30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### you could also predict from gaussian filtered face PCs\n",
    "### or from various timelags of the face PCs\n",
    "### this might improve prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for extra points, now redo everything with TEST and  TRAINING data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
